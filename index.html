<!doctype html>
<html lang="en">

<head>
  <script defer data-domain="nextgen-aisafety-workshop.cc" src="https://plausible.io/js/script.js"></script>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="/nextgenaisafety-logo.jpeg">
  <link rel="image_src" href="/nextgenaisafety-logo.jpeg">

  <!-- Bootstrap CSS -->
  <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
  <!-- Bootstrap JS -->
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.bundle.min.js"></script>
  <link rel="icon" href="arrow.png" type="image/x-icon">

  <title>Next Gen AI Safety 2024 @ ICML Vienna</title>

  <style>
    strong {
      font-weight: 600;
    }

    .biglist>li {
      margin-top: 20px !important;
    }

    .biglist li {
      margin-top: 5px;
    }

    .card {
      background: none !important;
      border: none !important;
      padding-left: 10px;
      padding-right: 10px;
    }

    .card-body {
      text-align: center;
    }

    .card a:hover {
      text-decoration: none;
      color: darkslategray;
    }

    .card a {
      color: black;
    }

    .container {
      max-width: 1000px;
    }

    .lead {
      font-size: 14pt;
    }

    html {
      scroll-behavior: smooth;
    }

    .card-body {
      padding: 1.25rem 0.25rem;
    }
  </style>
</head>

<body>
  <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top">
    <div class="container">
      <a class="navbar-brand" href="#">NextGenAISafety 2024</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link" href="#call">Call for Papers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#dates">Important Dates</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#schedule">Schedule</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#organizers">Organizers</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>


  <!-- Heading -->
  <section class="text-center bg-light py-5">
    <br />
    <h1 class="display-4">NextGenAISafety 2024 @ ICML Vienna</h1>
    <p class="lead"><strong>ICML 2024 Workshop on the Next Generation of AI Safety</strong></p>
    <p class="lead">Friday, July 26, 2024</p>
    <p class="lead">Messe Wien Exhibition Congress Center, Hall A1</p>
    <p class="lead"><a href="https://docs.google.com/document/d/1EC__CB5zwV_EdPUpOm7ik43jtc8lmz_xDeZAQ8ihirg">Call for
        Papers</a></p>
    <p class="lead">Camera-ready deadline July 8 AoE on
      <a href="https://openreview.net/group?id=ICML.cc/2024/Workshop/NextGenAISafety">OpenReview</a>. Poster instructions from <a href="https://icml.cc/Conferences/2024/PosterInstructions">ICML</a>.
    </p>
    <p class="lead">Contact info: next-gen-ai-safety [at] googlegroups [dot] com</p>
  </section>

  <!-- Description -->
  <section class="py-5">
    <div class="container" id="about">
      <p class="lead" style="text-align: center; max-width: 600px; margin: 0 auto;">
        <em>
          What does the next frontier in AI safety look like? How do we evaluate it? And how can we develop strong
          safeguards for tomorrow's AI systems?
        </em>
      </p>
      <br />
      <p class="lead" style="text-align: justify">
        Combatting the novel challenges of next generation AI systems necessitates new safety techniques, spanning areas
        such as synthetic data generation and utilization, content moderation, and model training methodologies. The
        proliferation of open-source and personalized models tailored for various applications widens the scope of
        deployments, and amplifies the already-urgent need for robust safety tools. Moreover, this diverse range of
        potential deployments entails complex trade-offs between safety objectives and operational efficiency. Taken
        together, there is a broad set of urgent and unique research challenges and opportunities to ensure the safety
        of tomorrow's AI systems.
      </p>
      <p class="lead" style="text-align: justify">
        In this workshop, we take a proactive approach to safety and focus on five emerging trends in AI and explore the
        challenges associated with deploying these technologies safely:
      <ul class="biglist" style="text-align: justify">
        <li>
          <em>Agentic AI</em>: As AI agents become more autonomous, concerns about unintended consequences, ethical
          issues, and adversary exploitation emerge. How do we ensure these agents respect privacy, and adhere to safety
          protocols?
        </li>
        <li>
          <em>Multimodal</em>: With the evolution of AI systems to process and generate diverse modalities
          like audio, video, and images, concerns around content appropriateness, privacy, bias, and misinformation
          arise. How do we craft robust guidelines and security measures to tackle these challenges?
        </li>
        <li><em>Personalized Interactions</em>: As conversational agents evolve for social and personal
          interaction, risks like data privacy breaches and echo chambers grow. How do we balance tailored experiences
          with user safety?
        </li>
        <li><em>Sensitive Applications</em>: With AI's integration into high-risk domains like legal, medical, and
          mental health, the stakes rise with risks such as overreliance on automation and potential catastrophic
          errors. How do we ensure that AI systems in these critical areas enhance decision-making without
          compromising human expertise and judgment?
        </li>
        <li><em>Dangerous Capabilities</em>: As AI's knowledge and understanding capabilities improve, these systems
          could be leveraged to extract or generate information about harmful applications or technologies, including
          bioweapons or cyber attack methods. How do we ensure that AI systems are designed with safeguards to prevent
          their misuse in creating or disseminating dangerous knowledge, while still allowing for beneficial research
          and innovation?
        </li>

      </ul>
      </p>

      <p class="lead" style="text-align: justify">
        We will bring together researchers across academia and industry working on improving safety
        and alignment of state-of-the-art AI systems as they are deployed. We aim for the event to facilitate sharing of
        challenges, best practices, new research ideas, data, and evaluations, that both practically aid development and
        spur progress in this area.
      </p>

    </div>
  </section>




  <!-- Call for papers -->
  <section class="py-5">
    <div class="container lead" id="call">
      <h2 class="text-center mb-4">Call for Papers</h2>
      <h5 style="text-align: center;">
        <a href="https://openreview.net/group?id=ICML.cc/2024/Workshop/NextGenAISafety">
          Camera-ready versions due July 8th AoE!</a> Read the full Call for Papers <a
          href="https://docs.google.com/document/d/1EC__CB5zwV_EdPUpOm7ik43jtc8lmz_xDeZAQ8ihirg">here</a>.
      </h5>
    </div>
  </section>

  <!-- Submission instructions -->
  <section class="py-5">
    <div class="container lead" id="submissionInstructions" style="max-width: 600px;">
      <h2 class="text-center mb-4">Submission Instructions</h2>
      <ul>
        <li>
          To submit a paper, please follow the following instructions and policies.
          <ul>
            <li>NGAIS is non-archival (no proceedings), but accepted papers will be made public via <a
                href="https://openreview.net/group?id=ICML.cc/2024/Workshop/NextGenAISafety">OpenReview</a> and the ICML website.</li>
            <li>NGAIS is primarily an in-person event, and authors should be prepared to present a poster at the
              workshop.</li>
            <li>Submissions can be in-review or published at prior venues, but we will prioritize new submissions for
              presentation at the workshop.</li>
            <li>Papers should be up to 5 pages (excluding references and supplementary materials) in <a
                href="https://icml.cc/Conferences/2024/CallForPapers">ICML 2024 format</a>. <strong>For the final
                  camera-ready version of the paper, authors can add one extra page to the main body.</strong>
              Authors should anonymize their submissions, as our reviewing process is blinded.</li>
            <li>Authors should nominate at least one person to review submissions, with a reviewing load of at most 3
              papers.</li>
          </ul>
        </li>
        <li>
          When ready, submit to <a
            href="https://openreview.net/group?id=ICML.cc/2024/Workshop/NextGenAISafety">OpenReview</a> (opens May
          22nd).
        </li>
        <li>
          <strong>Poster instructions</strong> can be found <a href="https://icml.cc/Conferences/2024/PosterInstructions">here</a>.
         </li>
         <li>
           <strong>Accepted oral talks</strong> should be no more than 8 minutes in length.
         </li>

      </ul>
      <!-- Submit to OpenReview  -->
    </div>
  </section>


  <!-- Important dates -->
  <section class="py-5">
    <div class="container lead" id="dates" style="max-width: 600px;">
      <h2 class="text-center mb-4">Important Dates</h2>
      <!-- Add important dates here -->
      <br />
      <strong>May 22:</strong> Submission portal opens <br /><br />
      <strong>May 30 (AoE):</strong> Deadline for papers<br /><br />
      <strong>June 18:</strong> Decision notifications <br /><br />
      <strong>July 8 (AoE):</strong> Camera-ready deadline (for the final
                  camera-ready version of the paper, authors can add one extra page to the main body) <br /><br />
      <strong>July 26: </strong> Workshop!
    </div>
  </section>


  <!-- Schedule -->
  <section class="py-5">
    <div class="container" style="max-width: 600px;" id="schedule">
      <h2 class="text-center mb-4">Schedule</h2>
      <p class="lead">
        The workshop will be held on Friday, July 26, 2024, in the Messe Wien Exhibition Congress Center, Hall A1. All conference schedule times are in Central European Summer Time (Vienna local time).
      </p>


    </div>

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Conference Schedule</title>
        <style>
          body {
            font-family: 'Arial', sans-serif;
            margin: 0;
            padding: 0;
            background: #f9f9f9;
          }

          .schedule {
            max-width: 800px;
            margin: 20px auto;
            background: white;
            padding: 20px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
          }

          .schedule-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 10px;
            padding-bottom: 10px;
            border-bottom: 1px solid #eee;
            cursor: pointer;
          }

          .additional-content {
            display: none;
            padding: 10px;
            margin-top: 5px;
            border-top: 1px solid #eee;
          }

          .toggle-indicator {
            display: inline-block;
            margin-left: 10px;
            transition: transform 0.3s ease;
          }

          .toggle-indicator::after {
            content: '\25BC';
            /* Down-pointing triangle */
          }

          .schedule-item.open .toggle-indicator::after {
            content: '\25B2';
            /* Up-pointing triangle */
          }

          .time {
            font-weight: bold;
            min-width: 150px;
            /* Adjust the width as needed */
          }

          .event {
            flex-grow: 1;
            padding-left: 10px;
          }

          .event-title {
            font-weight: bold;
          }

          ul {
            list-style-type: none;
            padding-left: 20px;
            margin: 0;
          }

          ul li {
            margin-bottom: 5px;
          }
        </style>
      </head>

      <body>
        <div class="schedule">
          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">9:00 AM - 9:45 AM</div>
            <div class="event">
              <span class="event-title">Invited Talk: Kamalika Chaudhuri</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Title:</b> Privacy in Representation Learning: Measurement and Mitigation <br>
            <b>Bio:</b> Kamalika Chaudhuri is a Professor in the department of Computer Science and Engineering at University of California San Diego, and a Research Scientist in the FAIR team at Meta AI. Her research interests are in the foundations of trustworthy machine learning, which includes problems such as learning from sensitive data while preserving privacy, learning under sampling bias, and in the presence of an adversary. She is particularly interested in privacy-preserving machine learning, which addresses how to learn good models and predictors from sensitive data, while preserving the privacy of individuals.
          </div>

          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">9:45 AM - 10:30 AM</div>
            <div class="event">
              <span class="event-title">Invited Talk: Inioluwa Deborah Raji</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Title:</b> Safety by Any Other Name <br>
            <b>Bio:</b> Deborah Raji is a senior Trustworthy AI fellow at the Mozilla Foundation and a CS PhD student at University of California, Berkeley, who is interested in questions on AI auditing and evaluation. She works closely with civil society, investigative journalists, policymakers and corporations on various projects to investigate, assess and better understand AI deployments. Recently, she was named to Forbes 30 Under 30, MIT Tech Review 35 Under 35 Innovators, and TIME100 Most Influential in AI.
          </div>

          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">10:30 AM - 11:00 AM</div>
            <div class="event">
              <span class="event-title">Oral Session #1</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Papers:</b>
              <ul>
                <li> <strong>10:30 AM - 10:40 AM</strong>: Privacy Auditing of Large Language Models <em>(Ashwinee Panda, Xinyu Tang, Milad Nasr, Christopher A. Choquette-Choo, Prateek Mittal)</em>
                <li> <strong>10:40 AM - 10:50 AM</strong>: Alignment Calibration: Machine Unlearning for Contrastive Learning under Auditing <em>(Yihan Wang, Yiwei Lu, Guojun Zhang, Franziska Boenisch, Adam Dziedzic, Yaoliang Yu, Xiao-Shan Gao)</em>
                <li> <strong>10:50 AM - 11:00 AM</strong>: BELLS: A Framework Towards Future Proof Benchmarks for the Evaluation of LLM Safeguards <em>(Diego Dorn, Alexandre Variengien, Charbel-Raphael Segerie, Vincent Corruble)</em>
              </ul>
          </div>

          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">11:00 AM - 11:45 AM</div>
            <div class="event">
              <span class="event-title">Invited Talk: Joelle Pineau</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Title:</b> TBD <br>
            <b>Bio:</b> Joelle Pineau is a Professor and William Dawson Scholar at the School of Computer Science at McGill University, where she co-directs the Reasoning and Learning Lab. She is a core academic member of Mila and a Canada CIFAR AI chairholder. She is also a VP, AI research at Meta (previously Facebook), where she leads the Fundamental AI Research (FAIR) team. She holds a BASc in Systems Design Engineering from the University of Waterloo, and an MSc and PhD in Robotics from Carnegie Mellon University. Dr. Pineau's research focuses on developing new models and algorithms for planning and learning in complex partially-observable domains. She also works on applying these algorithms to complex problems in robotics, health care, games and conversational agents. She serves on the editorial board of the Journal of Machine Learning Research and is Past-President of the International Machine Learning Society. She is a recipient of NSERC's E.W.R. Steacie Memorial Fellowship (2018), a Fellow of the Association for the Advancement of Artificial Intelligence (AAAI), a Senior Fellow of the Canadian Institute for Advanced Research (CIFAR), a member of the College of New Scholars, Artists and Scientists by the Royal Society of Canada, and a 2019 recipient of the Governor General's Innovation Awards.
          </div>

          <div class="schedule-item">
            <div class="time">11:45 AM - 12:30 PM</div>
            
            <div class="event">Panel</div>
          </div>

          <div class="schedule-item">
            <div class="time">12:30 PM - 2:00 PM</div>
            <div class="event">Lunch</div>
          </div>

          <div class="schedule-item">
            <div class="time">2:00 PM - 3:00 PM</div>
            <div class="event">Poster Session #1 (odd-numbered accepted posters)</div>
          </div>

          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">3:00 PM - 3:30 PM</div>
            <div class="event">
              <span class="event-title">Oral Session #2</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Papers:</b>
              <ul>
                <li> <strong>3:00 PM - 3:10 PM</strong>: Exploiting LLM Quantization <em>(Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev)</em>
                <li> <strong>3:10 PM - 3:20 PM</strong>: Accuracy on the wrong line: On the pitfalls of noisy data for OOD generalisation <em>(Amartya Sanyal, Yaxi Hu, Yaodong Yu, Yian Ma, Yixin Wang, Bernhard Schölkopf)</em>
                <li> <strong>3:20 PM - 3:30 PM</strong>: Generating Potent Poisons and Backdoors from Scratch with Guided Diffusion <em>(Hossein Souri, Arpit Bansal, Hamid Kazemi, Liam H Fowl, Aniruddha Saha, Jonas Geiping, Andrew Gordon Wilson, Rama Chellappa, Tom Goldstein, Micah Goldblum)</em>
              </ul>
          </div>

          <div class="schedule-item">
            <div class="time">3:30 PM - 4:30 PM</div>
            <div class="event">Poster Session #2 (even-numbered accepted posters), coffee break</div>
          </div>

          <div class="schedule-item" onclick="toggleContent(this)">
            <div class="time">4:30 PM - 5:00 PM</div>
            <div class="event">
              <span class="event-title">Invited Talk: Lilian Weng</span>
              <span class="toggle-indicator"></span>
            </div>
          </div>
          <div class="additional-content">
            <b>Title:</b> Towards Safe AGI <br>
            <b>Bio:</b> Lilian is the head of Safety Systems at OpenAI, where she leads a group of engineers and researchers to enable safety deployment of our models and products. Previously, she led Applied Research to leverage LLMs to address real-world applications. In the early days of her OpenAI time, Lilian contributed to OpenAI’s Robotics team, tackling complex robotic manipulation tasks like solving a Rubik’s Cube using a robot hand. With a wide range of research interests, she shares her insights on diverse topics in deep learning through her popular ML blog https://lilianweng.github.io/.
          </div>
        </div>

        <script>
          function toggleContent(element) {
            var content = element.nextElementSibling;
            if (content.style.display === "block") {
              content.style.display = "none";
              element.classList.remove('open');
            } else {
              content.style.display = "block";
              element.classList.add('open');
            }
          }
        </script>
      </body>

  </section>



  <!-- Organizers -->
  <section class="py-5 bg-light">
    <div class="container" style="max-width: 600px;" id="organizers">
      <h2 class="text-center mb-4">Organizers</h2>
      <div class="row row-cols-3 row-cols-sm-3 justify-content-center">
        <div class="card">
          <a href="https://alexbeutel.com/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/alex.jpg" alt="Alex Beutel photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Alex Beutel</h5>
            </div>
          </a>
        </div>
        <div class="card">
          <a href="https://madry.mit.edu/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/aleksander.png" alt="Aleksander Mądry photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Aleksander Mądry</h5>
            </div>
          </a>
        </div>
        <div class="card">
          <a href="https://sites.google.com/view/beirami">
            <img class="card-img-top img-fluid rounded-circle" src="assets/ahmad.jpeg" alt="Ahmad Beirami photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Ahmad Beirami</h5>
            </div>
          </a>
        </div>
        <div class="card">
          <a href="https://ai.meta.com/people/adina-williams/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/adina.jpeg" alt="Adina Williams photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Adina Williams</h5>
            </div>
          </a>
        </div>
        <div class="card">
          <a href="https://scholar.google.com/citations?user=v2cMiCAAAAAJ">
            <img class="card-img-top img-fluid rounded-circle" src="assets/beyza.png" alt="Beyza Ermis photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Beyza Ermis</h5>
            </div>
          </a>
        </div>

        <div class="card">
          <a href="https://scholar.google.com/citations?user=FRBObOwAAAAJ">
            <img class="card-img-top img-fluid rounded-circle" src="assets/ian.jpg" alt="Ian Kivlichan photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Ian Kivlichan</h5>
            </div>
          </a>
        </div>

        <div class="card">
          <a href="http://plahoti.de/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/preethi.jpeg" alt="Preethi Lahoti photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Preethi Lahoti</h5>
            </div>
          </a>
        </div>

        <div class="card">
          <a href="https://shibanisanturkar.com/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/shibani.png" alt="Shibani Santurkar photo"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Shibani Santurkar</h5>
            </div>
          </a>
        </div>

        <div class="card">
          <a href="https://thashim.github.io/">
            <img class="card-img-top img-fluid rounded-circle" src="assets/tatsu.jpg" alt="Tatsunori Hashimoto"
              width="250">
            <div class="card-body">
              <h5 class="card-title">Tatsunori Hashimoto</h5>
            </div>
          </a>
        </div>


      </div>
    </div>
  </section>



  <!-- Program Committee 
  <section class="py-5 bg-light">
    <div class="container" style="max-width: 600px;" id="programcom">
      <h2 class="text-center mb-4">Program Committee</h2>
      TBC
    </div>
  </section>
  -->



  <!-- Optional JavaScript; choose one of the two! -->

  <!-- Option 1: jQuery and Bootstrap Bundle (includes Popper) -->
  <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.bundle.min.js"></script>
</body>

</html>
